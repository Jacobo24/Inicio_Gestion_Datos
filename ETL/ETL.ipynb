{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceso ETL: Extracción desde Azure y Carga en SQL Server Local\n",
    "\n",
    "Este Notebook realiza un proceso ETL automático que extrae datos desde Azure SQL Database, los transforma con Pandas y los carga en SQL Server Local. Se trabajan las siguientes tablas: **Hechos**, **clientes**, **zona**, **tiempo** y **productos**.\n",
    "\n",
    "Las etapas del proceso son:\n",
    "1. **Configuración de Conexiones:** Se definen las conexiones para Azure y SQL Server Local.\n",
    "2. **Definición de Consultas y Metadatos:** Se establecen las rutas a los archivos SQL de extracción y se definen las claves primarias y foráneas para cada tabla.\n",
    "3. **Funciones Auxiliares:**\n",
    "    - Función para crear tablas dinámicamente en SQL Server Local.\n",
    "    - Función para eliminar tablas existentes en un orden que respete las dependencias.\n",
    "4. **Ejecución del Proceso ETL:** Se extraen, transforman y cargan los datos en el Data Warehouse local.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc         # Conexión con bases de datos SQL Server\n",
    "import pandas as pd   # Manipulación y transformación de datos\n",
    "import numpy as np    # Gestión de datos numéricos\n",
    "import os             # Operaciones con archivos y rutas\n",
    "import warnings       # Control de advertencias\n",
    "\n",
    "# Suprimir avisos innecesarios\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURACIÓN DE CONEXIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# CONFIGURACIÓN DE CONEXIONES\n",
    "# -------------------------\n",
    "\n",
    "# Conexión a Azure SQL Database\n",
    "AZURE_SERVER = 'uaxmathfis.database.windows.net'\n",
    "AZURE_DATABASE = 'usecases'\n",
    "AZURE_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "azure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\"\n",
    "\n",
    "# Conexión a SQL Server Local\n",
    "LOCAL_SERVER = 'localhost'\n",
    "LOCAL_DATABASE = 'dwh_case1'\n",
    "LOCAL_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "local_conn_str = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINICIÓN DE CONSULTAS Y METADATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# DEFINICIÓN DE TABLAS Y ARCHIVOS SQL\n",
    "# -------------------------\n",
    "\n",
    "# Ubicación de los archivos SQL con las consultas de extracción\n",
    "query_folder = \"../Modelo_dimensional\"  # Actualiza esta ruta según tu estructura\n",
    "\n",
    "queries = {\n",
    "    \"dim_geo\": \"Zona.sql\",\n",
    "    \"dim_product\": \"Productos.sql\",\n",
    "    \"dim_time\": \"Tiempo.sql\",\n",
    "    \"dim_client\": \"Clientes.sql\",\n",
    "    \"fact_sales\": \"Hechos.sql\"\n",
    "}\n",
    "\n",
    "# Definición de claves primarias (actualiza los nombres según corresponda)\n",
    "primary_keys = {\n",
    "    \"fact_sales\": [\"CODE\"],   # Se usa la columna \"CODE\" como identificador único\n",
    "    \"dim_client\": [\"Customer_ID\"],\n",
    "    \"dim_geo\": [\"TIENDA_ID\"],\n",
    "    \"dim_product\": [\"Id_Producto\"],\n",
    "    \"dim_time\": [\"Date\"]\n",
    "}\n",
    "\n",
    "# Definición de claves foráneas para fact_sales (ajusta los nombres si es necesario)\n",
    "foreign_keys = {\n",
    "    \"fact_sales\": {\n",
    "        \"Customer_ID\": \"dim_client(Customer_ID)\",\n",
    "        \"TIENDA_ID\": \"dim_geo(TIENDA_ID)\",  # Se asume que en zona se identifica mediante TIENDA_ID\n",
    "        \"Id_Producto\": \"dim_product(Id_Producto)\",\n",
    "        \"Sales_Date\": \"dim_time(Date)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Diccionario global para columnas de fecha (ajusta los nombres según tus datos)\n",
    "date_columns_global = {\n",
    "    \"dim_client\": [\"Fecha_nacimiento\"],\n",
    "    \"dim_time\": [\"inicio_de_mes\", \"fin_de_mes\", \"fecha\"],\n",
    "    \"fact_sales\": [\"DATE_ULTIMA_REVISION\", \"Logistic_date\", \"Prod_date\", \"fecha_venta\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCIONES AUXILIARES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCIÓN PARA CREAR TABLAS EN SQL SERVER LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_sql(table_name, df):\n",
    "    date_columns = date_columns_global.get(table_name, [])\n",
    "    col_defs = []\n",
    "    for col in df.columns:\n",
    "        if col in date_columns:\n",
    "            col_defs.append(f'[{col}] DATE')\n",
    "        elif np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            col_defs.append(f'[{col}] DATE')\n",
    "        elif df[col].dtype == np.float32:\n",
    "            col_defs.append(f'[{col}] FLOAT')\n",
    "        elif df[col].dtype == np.int32:\n",
    "            col_defs.append(f'[{col}] INT')\n",
    "        else:\n",
    "            max_len = df[col].astype(str).str.len().max()\n",
    "            varchar_size = min(2000, max(1, int(max_len * 1.3)))\n",
    "            col_defs.append(f'[{col}] NVARCHAR({varchar_size})')\n",
    "    pk = \", PRIMARY KEY (\" + \", \".join(primary_keys[table_name]) + \")\" if table_name in primary_keys else \"\"\n",
    "    fk = \"\"\n",
    "    if table_name in foreign_keys:\n",
    "        for col, ref in foreign_keys[table_name].items():\n",
    "            fk += f\", FOREIGN KEY ({col}) REFERENCES {ref}\"\n",
    "    return f\"CREATE TABLE {table_name} ({', '.join(col_defs)}{pk}{fk});\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para Eliminar Tablas en Orden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables_in_order(cursor, conn):\n",
    "    # Se elimina primero la tabla de hechos y luego las dimensiones, para respetar dependencias.\n",
    "    drop_order = [\"fact_sales\", \"dim_time\", \"dim_product\", \"dim_geo\", \"dim_client\"]\n",
    "    for table in drop_order:\n",
    "        check_exists_query = f\"IF OBJECT_ID('{table}', 'U') IS NOT NULL DROP TABLE {table};\"\n",
    "        try:\n",
    "            cursor.execute(check_exists_query)\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error al eliminar la tabla {table}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJECUCIÓN DEL PROCESO ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexiones establecidas correctamente.\n",
      "Tablas existentes eliminadas correctamente.\n",
      "\n",
      "Procesando: dim_geo\n",
      "Columnas en dim_geo: ['TIENDA_ID', 'ZONA', 'TIENDA_DESC', 'PROVINCIA_ID', 'PROV_DESC']\n",
      "Tabla 'dim_geo' creada en SQL Server Local.\n",
      "12 registros insertados en 'dim_geo'.\n",
      "\n",
      "Procesando: dim_product\n",
      "Columnas en dim_product: ['Id_Producto', 'Modelo', 'CATEGORIA_ID', 'Equipamiento', 'Fuel_ID', 'FUEL', 'id_transmision', 'TIPO_CARROCERIA', 'Kw', 'cantidad_vendida', 'pvp_total']\n",
      "Tabla 'dim_product' creada en SQL Server Local.\n",
      "404 registros insertados en 'dim_product'.\n",
      "\n",
      "Procesando: dim_time\n",
      "Columnas en dim_time: ['Date', 'Anno', 'Mes', 'Mes_desc', 'Week', 'Dia', 'Diadelasemana', 'Diadelesemana_desc', 'Festivo', 'Findesemana', 'Laboral', 'InicioMes', 'FinMes']\n",
      "Tabla 'dim_time' creada en SQL Server Local.\n",
      "3652 registros insertados en 'dim_time'.\n",
      "\n",
      "Procesando: dim_client\n",
      "Columnas en dim_client: ['Customer_ID', 'CODIGO_POSTAL', 'Edad', 'GENERO', 'RENTA_MEDIA_ESTIMADA', 'STATUS_SOCIAL', 'Fecha_nacimiento', 'ENCUESTA_CLIENTE_ZONA_TALLER', 'ENCUESTA_ZONA_CLIENTE_VENTA', 'poblacion', 'provincia', 'Max_Mosaic', 'Max_Mosaic_G', 'Renta_Media', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
      "Tabla 'dim_client' creada en SQL Server Local.\n",
      "44053 registros insertados en 'dim_client'.\n",
      "\n",
      "Procesando: fact_sales\n",
      "Columnas en fact_sales: ['CODE', 'TIENDA_ID', 'Customer_ID', 'Id_Producto', 'Date', 'Sales_Date', 'PVP', 'MANTENIMIENTO_GRATUITO', 'SEGURO_BATERIA_LARGO_PLAZO', 'FIN_GARANTIA', 'COSTE_VENTA_NO_IMPUESTOS', 'IMPUESTOS', 'EN_GARANTIA', 'EXTENSION_GARANTIA', 'Margen', 'Margendistribuidor', 'Costetransporte', 'GastosMarketing', 'Comisión_marca', 'Lead_compra', 'fue_Lead', 'DIAS_DESDE_ULTIMA_REVISION', 'Car_Age', 'km_ultima_revision', 'Margen_eur_bruto', 'Margen_eur', 'Churn']\n",
      "Tabla 'fact_sales' creada en SQL Server Local.\n",
      "58049 registros insertados en 'fact_sales'.\n",
      "ETL COMPLETADO.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Establecer conexiones con Azure SQL y SQL Server Local\n",
    "    conn_azure = pyodbc.connect(azure_conn_str)\n",
    "    conn_local = pyodbc.connect(local_conn_str)\n",
    "    print(\"Conexiones establecidas correctamente.\")\n",
    "    \n",
    "    # Eliminar tablas existentes en el orden correcto\n",
    "    with conn_local.cursor() as cursor:\n",
    "        drop_tables_in_order(cursor, conn_local)\n",
    "        print(\"Tablas existentes eliminadas correctamente.\")\n",
    "    \n",
    "    # Procesar cada tabla definida en el diccionario 'queries'\n",
    "    for table_name, file in queries.items():\n",
    "        print(f\"\\nProcesando: {table_name}\")\n",
    "        query_path = os.path.join(query_folder, file)\n",
    "        \n",
    "        with open(query_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sql_query = f.read()\n",
    "        \n",
    "        # Extraer datos desde Azure SQL\n",
    "        df = pd.read_sql(sql_query, conn_azure)\n",
    "        print(f\"Columnas en {table_name}: {df.columns.tolist()}\")  # Para depuración\n",
    "        \n",
    "        # Convertir columnas de fecha a datetime según corresponda\n",
    "        for col in date_columns_global.get(table_name, []):\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        \n",
    "        # Validar y eliminar duplicados según la clave primaria\n",
    "        if table_name in primary_keys:\n",
    "            pk_cols = primary_keys[table_name]\n",
    "            if all(col in df.columns for col in pk_cols):\n",
    "                df = df.drop_duplicates(subset=pk_cols)\n",
    "            else:\n",
    "                print(f\"Advertencia: La clave primaria {pk_cols} no se encuentra en {table_name}. Columnas disponibles: {df.columns.tolist()}\")\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"La consulta para {table_name} no devolvió datos.\")\n",
    "            continue\n",
    "        \n",
    "        # Rellenar valores nulos en columnas (excepto fechas)\n",
    "        for col in df.columns:\n",
    "            if col not in date_columns_global.get(table_name, []):\n",
    "                if df[col].dtype in [np.float64, np.int64]:\n",
    "                    df[col] = df[col].fillna(0)\n",
    "                else:\n",
    "                    df[col] = df[col].fillna(\"\")\n",
    "        \n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "        \n",
    "        # Crear la tabla en SQL Server Local\n",
    "        with conn_local.cursor() as cursor:\n",
    "            create_sql = create_table_sql(table_name, df)\n",
    "            cursor.execute(create_sql)\n",
    "            conn_local.commit()\n",
    "            print(f\"Tabla '{table_name}' creada en SQL Server Local.\")\n",
    "            \n",
    "            # Insertar los datos\n",
    "            columns = \", \".join([f\"[{col}]\" for col in df.columns])\n",
    "            placeholders = \", \".join(['?' for _ in df.columns])\n",
    "            insert_sql = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "            cursor.fast_executemany = True\n",
    "            cursor.executemany(insert_sql, df.values.tolist())\n",
    "            conn_local.commit()\n",
    "            print(f\"{df.shape[0]} registros insertados en '{table_name}'.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error en el proceso ETL: {e}\")\n",
    "\n",
    "finally:\n",
    "    conn_azure.close()\n",
    "    conn_local.close()\n",
    "\n",
    "print(\"ETL COMPLETADO.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen del Proceso ETL\n",
    "\n",
    "- **Extracción:** Se obtienen datos de Azure SQL Database mediante consultas definidas en archivos SQL (para las tablas: hechos, clientes, zona, tiempo y productos).\n",
    "- **Transformación:** Los datos se manipulan en Pandas, se convierten tipos de datos, se gestionan valores nulos y se prepara la estructura de las tablas.\n",
    "- **Carga:** Se crean las tablas en SQL Server Local respetando claves primarias y foráneas y se insertan los registros.\n",
    "\n",
    "Este Notebook automatiza el proceso ETL y permite monitorizar cada paso a través de los mensajes en consola."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-django",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
